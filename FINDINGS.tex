\documentclass{article} % This line defines the type of document. 'article' is a common class for small documents.
\usepackage[margin=1.3in]{geometry}
\usepackage{graphicx}
\usepackage{titlesec}
\usepackage{caption}
% \usepackage{hyperref}
\usepackage[colorlinks=true, linkcolor=blue, citecolor=blue, urlcolor=blue]{hyperref}
\usepackage{amsmath}

% \emergencystretch=3em

% Set paragraph indentation to zero
\setlength{\parindent}{0pt}

\titleformat{\section}
  {\normalfont\large\bfseries}{\thesection}{1em}{}

\begin{document} % This line marks the beginning of the document content.


\noindent\makebox[\linewidth]{\rule{\textwidth}{1pt}} 
\vspace*{0mm} % adds vertical space before the title
\begin{center}
    \Large\textbf{Toy Models of Superposition Replication and Findings}
\end{center}
\vspace*{2mm} % adds vertical space before the title
\noindent\makebox[\linewidth]{\rule{\textwidth}{1pt}}
\vspace*{0mm}

% \noindent
\noindent\textbf{Zephaniah Roe} \hfill \texttt{zroe@uchicago.edu}\\
\noindent\text{Undergraduate Student at the University of Chicago} \\


% \hfill
% \begin{minipage}[b]{0.4\linewidth}
%     \raggedleft
%     \texttt{G.RAS@DONDERS.RU.NL}
% \end{minipage}

% \vspace{2cm} % adds some vertical space before the main content


\begin{abstract}
\begin{quote}
    \textit{Toy Models of Superposition}\cite{elhage2022toy} is a groundbreaking paper published by 
    researchers affilated with Anthropic and Harvard University in 2022. By 
    investigating small models with under 100 neurons, the paper demonstrates 
    that neural networks can represent more features than they have demensions. 
    Additionally, they use these so called ``toy models'' to understand the 
    relationship between how neural networks are trained and how they represent 
    the data internally. The original paper is quite extensive. As a result, this
    replication focuses on reproducing the most important results from the  
    introduction and sections 2 and 3 of the original paper. It also includes some
    comentary on section 1.
\end{quote}
\end{abstract}
\section{Introduction}
The orginal paper motivates the idea of superposition with the following graphic:

\begin{figure}[h]
    \centering
    \includegraphics[width=0.55\linewidth]{section_1/images/section1_anthropic_graphic_.png}
    \captionsetup{font=footnotesize} % Set the font size for this caption
    % \caption{Graphic from Anthropic Paper}
    \label{fig:section1_anthropic}
\end{figure}

The basic idea is this: if you think of each feature as being represented inside of a
neural network by a direction, you can graph these directions and observe them.
By doing this, the authors of the original paper demonstrate that the way
a model maps features as directions depends on the sparsity of it's training data.
A replication of this phenomenon can be found below and the code used to 
generate it can be found 
\href{https://github.com/zroe1/toy_models_of_superposition/blob/main/section_1/section_1.ipynb}{here}.\\ 

\begin{figure}[h]
    \centering
    \includegraphics[width=0.6\linewidth]{section_1/images/section1_replicated_graphic.png}
    \captionsetup{font=footnotesize} % Set the font size for this caption
    \caption{Graphing superposition in 2D.}
    \label{fig:section1_replication}
\end{figure}

The model studied in  Figure~\ref{fig:section1_replication} is designed such that 
each column in the weight matrix corresponds to a  given input. Becasuse the weight
matrix only represents 2 neurons, the columns of the matrix can be graphed in
2D. As a result, it is trivial to plot the columns as 2D vectors (each representing 
individual features of the input). Obersving these vectors while increasing the 
sparsity of the model's input reveals
that the model can be trained to represent many more features than it has dimensions
(despite having only 2 neurons, the model in Figure~\ref{fig:section1_replication} 
can represent up to 5 features!). This is what the authors call "superposition." In 
future sections we will study superposition extensivly and produce the phenomenon
in larger networks.

\section{Background and Motivation}
In this section of \textit{Toy Models of Superposition}, the authors
provide context and define terms.  In this paper, I make a few additional comments 
about some of the key ideas from this section. \newline \newline
\textbf{(1) Defining Features: }\textit{Toy Models of Superposition}
defines features broadly as ``properties of the input which a sufficiently large 
neural network will reliably dedicate a neuron to representing.'' The authors do
however describe this definition as ``slightly circular'' and note that they are
not ``overly attached to it.'' I find the definition especially problematic because a network that is small or
has unconventional architecture may represent a feature that a larger network
or a network with a more typical architecture may ignore. These 
representations are clearly still features, but are not treated as so under the
original definition.\newline\newline
As a result, I propose an alternative definition: features are aspects of the
input that a neural network represents accurately with a significantly higher probability than 
a randomly initialized network. In other words, features are parts of the input 
that a model determines to be important enough to represent internally.\newline\newline
\textbf{(2) Role of Linear Representations in Neural Networks: }The original authors
of the paper study interpretability by trying to understand the linear representations
within neural networks. It is worth noting that this isn't the only way to approach
mechanistic interpretability research. Understanding the role of non-linearities 
at each level is likely also very important (and perhaps more neglected).\newline\newline
\textbf{(3) Defining Superposition: } The original paper has a compelling yet
simple definition for Superposition: ``Roughly, the idea of 
superposition is that neural networks `want to represent more features than they 
have neurons', so they exploit a property of high-dimensional spaces to 
simulate a model with many more neurons.'' This is the definition I will use
throughout this paper.

\section{Demonstrating Superposition}

In the introduction, the authors of the original paper proved that models with 
two neurons could exhibit superposition (this result was reproduced in Figure~\ref{fig:section1_replication}).
In this section, however, the authors demonstrate that superposition
is also observed in models with more than two neurons. \\

Specifically, they begin by exploring models with 20 inputs and 5 neurons, 
ultimately proving that these models exhibit superposition under certain 
conditions. The authors demonstrate this by graphing $W^TW$ for the weight 
matrix $W$ in each model (shown in Figure~\ref{fig:section3_anthropic}). 
They represent positive numbers in the matrix as red and negative ones as blue. They also graph the length 
of each feature by treating each column in $W$ as a vector. Features that are 
orthogonal to others in $W$ are labeled black while features that aren't are labeled yellow
(the exact details for how this is calculated is discussed in \ref{sec:calc_super}).\\

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{demonstrating_superposition/images/anthropic_section3.png}
    \captionsetup{font=footnotesize} % Set the font size for this caption
    \caption{Graphs linear and ReLU models from \textit{Toy Models of Superposition}\cite{elhage2022toy}}
    \label{fig:section3_anthropic}
\end{figure}

In Figure~\ref{fig:section3_anthropic}, the authors study both a linear model
and a model with a ReLU activation function. They found that both the linear
and ReLU model did not exhibit superposition in the absence of sparsity. By 
increasing the sparsity of the input, however, the ReLU model begins to clearly 
exhibit superposition by ceasing to represent features orthogonally.\\ 

The first step in replicating these findings was to train the linear and ReLU
models that don't perform computation in superposition. The linear model was
defined by $W^TWx + b$ and the ReLU model was defined by ReLU($W^TWx + b$). The
objective of each model was to reconstruct the input $x$. Both 
models were trained with the Adam optimizer (learning rate = $1*10^{-3}$) on 
20,000 batches of 256 examples.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.4\linewidth]{demonstrating_superposition/images/relu_linear_0_sparsity.png}
    \captionsetup{font=footnotesize, width=0.7\linewidth} % Set the font size for this caption
    \caption{The generated graphics show that the model uses each of its five 
    dimensions to graph the 5 most important features orthogonally.}
    \label{fig:relu_linear_0}
\end{figure}

Note that in Figure~\ref{fig:relu_linear_0}, I use orange to indicate
positive numbers and purple to indicate negative ones. This is different from
the red and blue in Figure~\ref{fig:section3_anthropic}, to distinguish my work 
from that of the original authors. Similarly, while the original authors use 
yellow to indicate features in superposition, I use blue (This is hard to see in 
Figure~\ref{fig:relu_linear_0} but it will be more obvious going forward).

\subsection{Calculating Superposition}
\label{sec:calc_super}

The models in Figure~\ref{fig:relu_linear_0} do not exhibit superposition. They
encode the five most important features orthogonally (one feature for each
neuron in the model). In this section, we will be investigating models
that do not behave in this way, instead encoding features as vectors that interfere
with eachother. The model explored in this section have the same architecture
and objective as the models in Figure~\ref{fig:relu_linear_0}. The difference
is that the models in this section are trained on sparse input data and, as a
result, map features interally in superposition. \\

In order to explain this phenomenon and demonstate how models with sparse input
are able to represent features in superpostion, it will be useful to dive into 
the math behind the concept of feature interference. The extent to which 
features interfere with eachother is defined by the following equation:

\begin{equation}
\label{eq:my_equation}
\text{Interference} = \sum_{j \neq i} (\hat{W}_i \cdot \hat{W}_j)^2
\end{equation}

For a given column $i$ in weight matrix $W$, interference is calculated by taking
the dot product with every other column in $W$. Non-zero dot products indicate
that the columns in $W$ are not orthogonal. As a result, summing these dot 
products gives a general idea of how much the network is representing a given
feature in superposition. Note that $\hat{W}_i$ is the unit vector for $W_i$. 
This is necessary because when calculating interference, we are interested in
the direction of a given feature, not its length.\newline

In Figure~\ref{fig:sparsity_1}, the length of a feature (calculated by taking the
length of the vector $W_i$) determines the width of the bars in the feature
graph (shown in the bottom half of the figure). The interference equation (Equation~\ref{eq:my_equation}) determines the
color of the columns: black indicates a low value for interference
while blue indicates a higher value. This means that blue bars show that a given feature is represented
in superposition while black bars indicate that the feature is mapped orthogonally.
    

\begin{figure}[h]
    \centering
    \includegraphics[width=0.75\linewidth]{demonstrating_superposition/images/sparsity_superposition1.png}
    \captionsetup{font=footnotesize, width=0.71\linewidth} % Set the font size for this caption
    \caption{
        Superposition is observed in models trained on 70\%, 90\% and 97\% 
        sparse inputs (code to generate these figures can be found 
        \href{https://github.com/zroe1/toy_models_of_superposition/blob/main/section_1/section_1.ipynb}{here}). 
        The 70\% and 90\% sparse models were trained on 50,000 batches of 256 
        examples with the ``RMSProp'' optimizer (learning rate = $10^{-2}$). The 
        97\% sparsity model was trained on 100,000 batches of 256 examples using 
        the Adam opimizer (learning rate = $10^{-2}$).
    }
    \label{fig:sparsity_1}
\end{figure}

Unlike the models with 0\% sparsity in Figure~\ref{fig:relu_linear_0}, the models
in Figure~\ref{fig:sparsity_1} have higher levels of sparsity and, as
a result, leverage superposition. The bottom half of Figure~\ref{fig:sparsity_1}
shows that these models represent far many more features than the models in
Figure~\ref{fig:relu_linear_0}, but by doing so, they are forced to represent many 
of their features in superposition. The models only have 5 neurons so if they 
``want'' to represent more than 5 features, they can't represent each feature 
orthogonally. This tradeoff is intuatively more attractive when the model is
trained on sparse inputs becase it is less likely that the model will be fed
a combination of inputs that cause feature representations to conflict (because
a significant percentage of the input is 0).


\subsection{Models Trained on Very Sparse Data}

As sparsity is increased to almost 100\% the models stop representing any features
orthogonally. This is displayed in Figure~\ref{fig:sparsity_2} where models are
trained on 99\%, 99.7\% and 99.9\% sparse inputs.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.75\linewidth]{demonstrating_superposition/images/sparsity_superposition2.png}
    \captionsetup{font=footnotesize, width=0.7\linewidth} % Set the font size for this caption
    \caption{
        When models are trained on sufficiently sparse data, all feature
        representations are in superposition. The models in the figure were
        trained on 100,000 batches of 256 examples using the Adam opimizer 
        (learning rate = $10^{-2}$).
    }
    \label{fig:sparsity_2}
\end{figure}

The representations of $W^{T}W$ of these very sparse models (shown in the top
half of Figure~\ref{fig:sparsity_2}) is far less clean than previous
representations we have seen. This is the same trend the original authors
found when increasing sparsity of these models (Figure~\ref{fig:section3_anthropic}
illustrates how the original authors displayed this visually). \\

The feature representations, shown in the bottom have of Figure~\ref{fig:sparsity_2},
are also consistent with the the findings of \textit{Toy Models of Superposition}.
Like the investigation from the original paper, these feature representations 
show no features mapped orthogonally (recall that features that interfere with
eachother are shown in blue).

\bibliographystyle{plain}
\bibliography{references}

\end{document} % This line marks the end of the document content.
